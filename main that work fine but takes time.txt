import os
import re
import json
import string
import requests
from bs4 import BeautifulSoup
from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
from transformers import pipeline

# ============================================
# LOAD ENVIRONMENT
# ============================================
load_dotenv()

SERPAPI_KEY = os.getenv("SERPAPI_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
SCRAPER_API_KEY = os.getenv("SCRAPER_API_KEY", "")

app = FastAPI()

# ============================================
# LOAD NLI MODEL (STABLE)
# ============================================
print("Loading MNLI model...")
nli_model = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=-1
)
print("MNLI ready.")


# ============================================
# INPUT MODEL
# ============================================
class InputData(BaseModel):
    url: str


# ============================================
# HELPERS
# ============================================
def clean_query(text):
    allowed = string.ascii_letters + string.digits + " -/"
    cleaned = "".join(c for c in text if c in allowed)
    return " ".join(cleaned.split()[:12])


def extract_claim(text):
    return re.sub(r"\s+", " ", text.strip())[:300]


# ============================================
# SERPAPI SEARCH (REPLACES GOOGLE CSE)
# ============================================
def serpapi_search(query):
    url = "https://serpapi.com/search"
    params = {
        "engine": "google",
        "q": query,
        "api_key": SERPAPI_KEY
    }

    try:
        r = requests.get(url, params=params)
        data = r.json()
        results = []

        for item in data.get("organic_results", []):
            results.append({
                "title": item.get("title", ""),
                "link": item.get("link", ""),
                "snippet": item.get("snippet", "")
            })

        return results

    except Exception as e:
        print("SerpAPI error:", e)
        return []


# ============================================
# HYBRID SCRAPER
# ============================================
def extract_visible_text(html):
    try:
        soup = BeautifulSoup(html, "html.parser")
        text = " ".join([p.text.strip() for p in soup.find_all("p")])
        return re.sub(r"\s+", " ", text)[:800]
    except:
        return ""


def scrape_page(url):
    print("Scraping:", url)

    # 1️⃣ ScrapeNinja (no key needed)
    try:
        r = requests.post("https://scrapeninja.net/api/scrape", json={"url": url})
        if r.status_code == 200:
            html = r.json().get("body", "")
            if html:
                return extract_visible_text(html)
    except:
        pass

    # 2️⃣ ScraperAPI (optional)
    if SCRAPER_API_KEY:
        try:
            r = requests.get(
                f"http://api.scraperapi.com?api_key={SCRAPER_API_KEY}&url={url}&render=true"
            )
            return extract_visible_text(r.text)
        except:
            pass

    # 3️⃣ BeautifulSoup fallback
    try:
        html = requests.get(url, timeout=5).text
        return extract_visible_text(html)
    except:
        return ""


# ============================================
# EVIDENCE PIPELINE
# ============================================
def retrieve_evidence(claim):
    cleaned = clean_query(claim)

    strict_query = (
        f"\"{cleaned}\" (site:cdc.gov OR site:who.int OR site:nih.gov "
        f"OR site:mayoclinic.org OR site:hopkinsmedicine.org OR site:yalemedicine.org)"
    )

    items = serpapi_search(strict_query)

    if not items:
        fallback = cleaned + " health science verified"
        items = serpapi_search(fallback)

    evidence = []
    for it in items:
        title = it.get("title", "")
        link = it.get("link", "")
        snippet = it.get("snippet", "")

        scraped = scrape_page(link)
        final_snippet = scraped if scraped.strip() else snippet

        evidence.append({
            "title": title,
            "link": link,
            "snippet": final_snippet
        })

    return evidence


# ============================================
# NLI DECISION
# ============================================
def ml_nli_label(claim, evidence_list):
    support = 0.0
    contra = 0.0

    for ev in evidence_list:
        snippet = ev["snippet"]
        if len(snippet) < 30:
            continue

        result = nli_model(
            snippet,
            candidate_labels=["supports", "contradicts", "unrelated"],
            hypothesis_template=f"This text {{}} the claim: '{claim}'."
        )

        lbl = result["labels"][0]
        score = float(result["scores"][0])

        if score > 0.35:
            if lbl == "supports":
                support += score
            elif lbl == "contradicts":
                contra += score

    if contra > support and contra > 0.5:
        return "MISINFORMATION"
    if support > contra and support > 0.5:
        return "REAL"
    return "UNCERTAIN"


# ============================================
# GEMINI
# ============================================
def gemini_reasoning(claim, evidence):
    ev_text = "\n".join(ev["snippet"] for ev in evidence)

    prompt = f"""
Fact-check the claim using only scientific reasoning.

CLAIM:
{claim}

EVIDENCE:
{ev_text if ev_text else "NO EVIDENCE"}

Respond with ONE WORD ONLY:
REAL
MISINFORMATION
UNCERTAIN
"""

    try:
        r = requests.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={GEMINI_API_KEY}",
            json={"contents": [{"parts": [{"text": prompt}]}]}
        )
        raw = r.json()

        try:
            txt = raw["candidates"][0]["content"]["parts"][0]["text"].upper()
        except:
            txt = json.dumps(raw).upper()

        for w in ["REAL", "MISINFORMATION", "UNCERTAIN"]:
            if w in txt:
                return w

    except:
        pass

    return "UNCERTAIN"


# ============================================
# LLAMA
# ============================================
def openrouter_reasoning(claim, evidence):
    ev_text = "\n".join(ev["snippet"] for ev in evidence)

    prompt = f"""
CLAIM:
{claim}

EVIDENCE:
{ev_text if ev_text else "NO EVIDENCE"}

Return ONE WORD:
REAL
MISINFORMATION
UNCERTAIN
"""

    try:
        r = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENROUTER_API_KEY}"},
            json={
                "model": "meta-llama/llama-3.1-70b-instruct",
                "messages": [{"role": "user", "content": prompt}]
            }
        )

        txt = r.json()["choices"][0]["message"]["content"].upper()

        for w in ["REAL", "MISINFORMATION", "UNCERTAIN"]:
            if w in txt:
                return w

    except:
        pass

    return "UNCERTAIN"


# ============================================
# ENSEMBLE
# ============================================
def ensemble(ml, gem, llama):
    votes = [ml, gem, llama]
    if votes.count("MISINFORMATION") >= 2:
        return "MISINFORMATION"
    if votes.count("REAL") >= 2:
        return "REAL"
    return "UNCERTAIN"


# ============================================
# ENDPOINT
# ============================================
@app.post("/detect")
def detect(data: InputData):

    claim = extract_claim(data.url)
    evidence = retrieve_evidence(claim)

    ml = ml_nli_label(claim, evidence)
    gem = gemini_reasoning(claim, evidence)
    llama = openrouter_reasoning(claim, evidence)

    final = ensemble(ml, gem, llama)
    trust = round((sum([ml == final, gem == final, llama == final]) / 3) * 100)

    return {
        "claim": claim,
        "ml_label": ml,
        "gemini_label": gem,
        "openrouter_label": llama,
        "final_label": final,
        "trust_score": trust,
        "evidence": evidence
    }
